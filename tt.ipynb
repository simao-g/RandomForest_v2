{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd20a38d",
   "metadata": {},
   "source": [
    "**Análise da performance do RF nos diiferentes datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3456448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando no dataset: dataset_1000_hypothyroid.csv\n",
      "1 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1002_ipums_la_98-small.csv\n",
      "2 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1004_synthetic_control.csv\n",
      "3 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1013_analcatdata_challenger.csv\n",
      "4 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1014_analcatdata_dmft.csv\n",
      "5 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1016_vowel.csv\n",
      "6 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1018_ipums_la_99-small.csv\n",
      "7 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1020_mfeat-karhunen.csv\n",
      "8 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1021_page-blocks.csv\n",
      "9 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1022_mfeat-pixel.csv\n",
      "10 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1023_soybean.csv\n",
      "11 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1039_hiva_agnostic.csv\n",
      "12 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1045_kc1-top5.csv\n",
      "13 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1049_pc4.csv\n",
      "14 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1050_pc3.csv\n",
      "15 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1056_mc1.csv\n",
      "16 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1059_ar1.csv\n",
      "17 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1061_ar4.csv\n",
      "18 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1064_ar6.csv\n",
      "19 de 50\n",
      "\n",
      "Treinando no dataset: dataset_1065_kc3.csv\n",
      "20 de 50\n",
      "\n",
      "Treinando no dataset: dataset_311_oil_spill.csv\n",
      "21 de 50\n",
      "\n",
      "Treinando no dataset: dataset_312_scene.csv\n",
      "22 de 50\n",
      "\n",
      "Treinando no dataset: dataset_316_yeast_ml8.csv\n",
      "23 de 50\n",
      "\n",
      "Treinando no dataset: dataset_38_sick.csv\n",
      "24 de 50\n",
      "\n",
      "Treinando no dataset: dataset_450_analcatdata_lawsuit.csv\n",
      "25 de 50\n",
      "\n",
      "Treinando no dataset: dataset_463_backache.csv\n",
      "26 de 50\n",
      "\n",
      "Treinando no dataset: dataset_757_meta.csv\n",
      "27 de 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:41: RuntimeWarning: invalid value encountered in less\n",
      "  left_mask = X < value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:42: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  right_mask = X >= value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:35: RuntimeWarning: invalid value encountered in less\n",
      "  left_mask = X[:, column] < value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:36: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  right_mask = X[:, column] >= value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\tree.py:152: RuntimeWarning: invalid value encountered in divide\n",
      "  self.outcome = np.bincount(targets[\"y\"], minlength=self.n_classes) / targets[\"y\"].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando no dataset: dataset_764_analcatdata_apnea3.csv\n",
      "28 de 50\n",
      "\n",
      "Treinando no dataset: dataset_765_analcatdata_apnea2.csv\n",
      "29 de 50\n",
      "\n",
      "Treinando no dataset: dataset_767_analcatdata_apnea1.csv\n",
      "30 de 50\n",
      "\n",
      "Treinando no dataset: dataset_865_analcatdata_neavote.csv\n",
      "31 de 50\n",
      "\n",
      "Treinando no dataset: dataset_867_visualizing_livestock.csv\n",
      "32 de 50\n",
      "\n",
      "Treinando no dataset: dataset_875_analcatdata_chlamydia.csv\n",
      "33 de 50\n",
      "\n",
      "Treinando no dataset: dataset_940_water-treatment.csv\n",
      "34 de 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:41: RuntimeWarning: invalid value encountered in less\n",
      "  left_mask = X < value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:42: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  right_mask = X >= value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:35: RuntimeWarning: invalid value encountered in less\n",
      "  left_mask = X[:, column] < value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:36: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  right_mask = X[:, column] >= value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\tree.py:152: RuntimeWarning: invalid value encountered in divide\n",
      "  self.outcome = np.bincount(targets[\"y\"], minlength=self.n_classes) / targets[\"y\"].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando no dataset: dataset_947_arsenic-male-bladder.csv\n",
      "35 de 50\n",
      "\n",
      "Treinando no dataset: dataset_949_arsenic-female-bladder.csv\n",
      "36 de 50\n",
      "\n",
      "Treinando no dataset: dataset_950_arsenic-female-lung.csv\n",
      "37 de 50\n",
      "\n",
      "Treinando no dataset: dataset_951_arsenic-male-lung.csv\n",
      "38 de 50\n",
      "\n",
      "Treinando no dataset: dataset_954_spectrometer.csv\n",
      "39 de 50\n",
      "\n",
      "Treinando no dataset: dataset_958_segment.csv\n",
      "40 de 50\n",
      "\n",
      "Treinando no dataset: dataset_962_mfeat-morphological.csv\n",
      "41 de 50\n",
      "\n",
      "Treinando no dataset: dataset_966_analcatdata_halloffame.csv\n",
      "42 de 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:41: RuntimeWarning: invalid value encountered in less\n",
      "  left_mask = X < value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:42: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  right_mask = X >= value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:35: RuntimeWarning: invalid value encountered in less\n",
      "  left_mask = X[:, column] < value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\base.py:36: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  right_mask = X[:, column] >= value\n",
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\tree.py:152: RuntimeWarning: invalid value encountered in divide\n",
      "  self.outcome = np.bincount(targets[\"y\"], minlength=self.n_classes) / targets[\"y\"].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando no dataset: dataset_968_analcatdata_birthday.csv\n",
      "43 de 50\n",
      "\n",
      "Treinando no dataset: dataset_971_mfeat-fourier.csv\n",
      "44 de 50\n",
      "\n",
      "Treinando no dataset: dataset_976_JapaneseVowels.csv\n",
      "45 de 50\n",
      "\n",
      "Treinando no dataset: dataset_978_mfeat-factors.csv\n",
      "46 de 50\n",
      "\n",
      "Treinando no dataset: dataset_980_optdigits.csv\n",
      "47 de 50\n",
      "\n",
      "Treinando no dataset: dataset_984_analcatdata_draft.csv\n",
      "48 de 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\mla\\ensemble\\tree.py:152: RuntimeWarning: invalid value encountered in divide\n",
      "  self.outcome = np.bincount(targets[\"y\"], minlength=self.n_classes) / targets[\"y\"].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando no dataset: dataset_987_collins.csv\n",
      "49 de 50\n",
      "\n",
      "Treinando no dataset: dataset_995_mfeat-zernike.csv\n",
      "50 de 50\n",
      "\n",
      "Resultados:\n",
      "                                    dataset  accuracy    recall  precision  \\\n",
      "0              dataset_1000_hypothyroid.csv  0.998232  0.987654   0.999049   \n",
      "1        dataset_1002_ipums_la_98-small.csv  0.892205  0.500000   0.446102   \n",
      "2        dataset_1004_synthetic_control.csv  0.977778  0.973804   0.951144   \n",
      "3   dataset_1013_analcatdata_challenger.csv  1.000000  1.000000   1.000000   \n",
      "4         dataset_1014_analcatdata_dmft.csv  0.778243  0.500000   0.389121   \n",
      "5                    dataset_1016_vowel.csv  0.973064  0.943691   0.920767   \n",
      "6        dataset_1018_ipums_la_99-small.csv  0.929891  0.540955   0.625011   \n",
      "7           dataset_1020_mfeat-karhunen.csv  0.973333  0.915778   0.915778   \n",
      "8              dataset_1021_page-blocks.csv  0.974406  0.945418   0.924235   \n",
      "9              dataset_1022_mfeat-pixel.csv  0.995000  0.979857   0.988375   \n",
      "10                 dataset_1023_soybean.csv  0.975490  0.913889   0.965262   \n",
      "11           dataset_1039_hiva_agnostic.csv  0.958202  0.527358   0.668452   \n",
      "12                dataset_1045_kc1-top5.csv  0.930233  0.487805   0.476190   \n",
      "13                     dataset_1049_pc4.csv  0.878719  0.636669   0.749631   \n",
      "14                     dataset_1050_pc3.csv  0.861111  0.534972   0.558279   \n",
      "15                     dataset_1056_mc1.csv  0.993660  0.665603   0.747878   \n",
      "16                     dataset_1059_ar1.csv  0.944444  0.500000   0.472222   \n",
      "17                     dataset_1061_ar4.csv  0.812500  0.622857   0.747126   \n",
      "18                     dataset_1064_ar6.csv  0.766667  0.549689   0.642857   \n",
      "19                     dataset_1065_kc3.csv  0.897810  0.633685   0.687066   \n",
      "20                dataset_311_oil_spill.csv  0.950178  0.625253   0.651961   \n",
      "21                    dataset_312_scene.csv  0.962604  0.929916   0.932874   \n",
      "22                dataset_316_yeast_ml8.csv  0.973793  0.536057   0.547874   \n",
      "23                      dataset_38_sick.csv  0.990274  0.971672   0.937985   \n",
      "24      dataset_450_analcatdata_lawsuit.csv  0.962025  0.826484   0.886486   \n",
      "25                 dataset_463_backache.csv  0.777778  0.559783   0.559783   \n",
      "26                     dataset_757_meta.csv  0.094937  0.500000   0.047468   \n",
      "27       dataset_764_analcatdata_apnea3.csv  0.918519  0.877866   0.804907   \n",
      "28       dataset_765_analcatdata_apnea2.csv  0.950704  0.882901   0.929508   \n",
      "29       dataset_767_analcatdata_apnea1.csv  0.943662  0.888233   0.888233   \n",
      "30      dataset_865_analcatdata_neavote.csv  0.900000  0.500000   0.450000   \n",
      "31    dataset_867_visualizing_livestock.csv  0.871795  0.810268   0.780242   \n",
      "32    dataset_875_analcatdata_chlamydia.csv  0.233333  0.500000   0.116667   \n",
      "33          dataset_940_water-treatment.csv  1.000000  1.000000   1.000000   \n",
      "34     dataset_947_arsenic-male-bladder.csv  0.976048  0.800000   0.987578   \n",
      "35   dataset_949_arsenic-female-bladder.csv  0.844311  0.692953   0.641162   \n",
      "36      dataset_950_arsenic-female-lung.csv  0.982036  0.931211   0.885724   \n",
      "37        dataset_951_arsenic-male-lung.csv  1.000000  1.000000   1.000000   \n",
      "38             dataset_954_spectrometer.csv  0.943396  0.775369   0.843366   \n",
      "39                  dataset_958_segment.csv  0.991342  0.975097   0.984811   \n",
      "40      dataset_962_mfeat-morphological.csv  0.993333  0.970241   0.987273   \n",
      "41   dataset_966_analcatdata_halloffame.csv  0.079602  0.500000   0.039801   \n",
      "42     dataset_968_analcatdata_birthday.csv  0.853211  0.706989   0.706989   \n",
      "43            dataset_971_mfeat-fourier.csv  0.993333  0.978944   0.978944   \n",
      "44           dataset_976_JapaneseVowels.csv  0.974230  0.953570   0.952831   \n",
      "45            dataset_978_mfeat-factors.csv  0.990000  0.977120   0.961131   \n",
      "46                dataset_980_optdigits.csv  0.975089  0.911889   0.949115   \n",
      "47        dataset_984_analcatdata_draft.csv  0.954128  0.500000   0.477064   \n",
      "48                  dataset_987_collins.csv  1.000000  1.000000   1.000000   \n",
      "49            dataset_995_mfeat-zernike.csv  0.990000  0.951011   0.985054   \n",
      "\n",
      "    f1_score   roc_auc  \n",
      "0   0.993319  0.987748  \n",
      "1   0.471516  0.734997  \n",
      "2   0.962341  0.988309  \n",
      "3   1.000000       NaN  \n",
      "4   0.437647  0.508521  \n",
      "5   0.932088  0.994575  \n",
      "6   0.579953  0.742429  \n",
      "7   0.915778  0.883984  \n",
      "8   0.934707  0.959626  \n",
      "9   0.984097  0.981682  \n",
      "10  0.938873  0.960185  \n",
      "11  0.589581  0.513010  \n",
      "12  0.481928  0.292683  \n",
      "13  0.688548  0.726213  \n",
      "14  0.546377  0.724758  \n",
      "15  0.704346  0.575387  \n",
      "16  0.485714  0.588235  \n",
      "17  0.679356  0.582857  \n",
      "18  0.592634  0.652174  \n",
      "19  0.659297  0.772333  \n",
      "20  0.638327  0.757912  \n",
      "21  0.931393  0.948914  \n",
      "22  0.541901  0.631954  \n",
      "23  0.954531  0.962402  \n",
      "24  0.855434  0.952055  \n",
      "25  0.559783  0.665761  \n",
      "26  0.086705  0.606061  \n",
      "27  0.839805  0.930708  \n",
      "28  0.905605  0.964560  \n",
      "29  0.888233  0.875246  \n",
      "30  0.473684  0.506173  \n",
      "31  0.794971  0.776786  \n",
      "32  0.189189  0.521739  \n",
      "33  1.000000  1.000000  \n",
      "34  0.883947  0.747134  \n",
      "35  0.666052  0.742356  \n",
      "36  0.907898  0.869497  \n",
      "37  1.000000  1.000000  \n",
      "38  0.807940  0.907882  \n",
      "39  0.979930  0.974196  \n",
      "40  0.978683  0.967680  \n",
      "41  0.073733  0.488514  \n",
      "42  0.706989  0.822581  \n",
      "43  0.978944  0.963784  \n",
      "44  0.953200  0.962250  \n",
      "45  0.969060  0.980769  \n",
      "46  0.930130  0.910044  \n",
      "47  0.488263  0.534615  \n",
      "48  1.000000  1.000000  \n",
      "49  0.967733  0.949607  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mla.ensemble.random_forest import RandomForestClassifier  # Usando a tua implementação\n",
    "\n",
    "# Pasta dos datasets tratados\n",
    "pasta = r\"C:\\Users\\Utilizador\\Desktop\\AC1_Trabalho\\clean_class_imbalance\"\n",
    "\n",
    "# Funções para métricas sem sklearn\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    precisions = []\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        fp = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        if tp + fp == 0:\n",
    "            precisions.append(0)\n",
    "        else:\n",
    "            precisions.append(tp / (tp + fp))\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    recalls = []\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        fn = np.sum((y_pred != cls) & (y_true == cls))\n",
    "        if tp + fn == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            recalls.append(tp / (tp + fn))\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    if prec + rec == 0:\n",
    "        return 0\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def roc_auc(y_true, y_score):\n",
    "    # Só para binário\n",
    "    if len(np.unique(y_true)) != 2:\n",
    "        return np.nan\n",
    "\n",
    "    # Ordenar por score da classe positiva\n",
    "    order = np.argsort(-y_score[:, 1])\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    # Calcular TPR e FPR\n",
    "    P = np.sum(y_true == 1)\n",
    "    N = np.sum(y_true == 0)\n",
    "    tpr = np.cumsum(y_true_sorted == 1) / P\n",
    "    fpr = np.cumsum(y_true_sorted == 0) / N\n",
    "\n",
    "    # AUC pela regra do trapézio\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc\n",
    "\n",
    "# Função para dividir treino e teste sem sklearn\n",
    "def split_train_test(X, y, test_size=0.3, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    test_size = int(test_size * X.shape[0])\n",
    "    test_idx = indices[:test_size]\n",
    "    train_idx = indices[test_size:]\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "# Avaliação dos datasets\n",
    "resultados = []\n",
    "count = 1\n",
    "\n",
    "for arquivo in os.listdir(pasta):\n",
    "    if arquivo.endswith(\".csv\"):\n",
    "        caminho_arquivo = os.path.join(pasta, arquivo)\n",
    "        print(f\"\\nTreinando no dataset: {arquivo}\")\n",
    "        print(count, \"de 50\")\n",
    "        count +=1\n",
    "        \n",
    "        # Lê o dataset\n",
    "        df = pd.read_csv(caminho_arquivo)\n",
    "        \n",
    "        # Última coluna como target\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "\n",
    "        # Dividir treino e teste\n",
    "        X_train, X_test, y_train, y_test = split_train_test(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Treinar modelo\n",
    "        # Treinar modelo (com todos os atributos!)\n",
    "        modelo = RandomForestClassifier(\n",
    "            n_estimators=10,\n",
    "            max_depth=10,\n",
    "            max_features=X.shape[1]\n",
    "            )\n",
    "        modelo.fit(X_train, y_train)\n",
    "\n",
    "        # Prever\n",
    "        y_pred_prob = modelo._predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "        # Avaliar\n",
    "        acc = accuracy(y_test, y_pred)\n",
    "        rec = recall(y_test, y_pred)\n",
    "        prec = precision(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc(y_test, y_pred_prob)\n",
    "\n",
    "        resultados.append({\n",
    "            \"dataset\": arquivo,\n",
    "            \"accuracy\": acc,\n",
    "            \"recall\": rec,\n",
    "            \"precision\": prec,\n",
    "            \"f1_score\": f1,\n",
    "            \"roc_auc\": auc\n",
    "        })\n",
    "\n",
    "# Mostrar resultados\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "print(\"\\nResultados:\")\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3866",
   "metadata": {},
   "source": [
    "**Análise da perceentagem de outliers nos datasets Grupo 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets analyzed: 29\n",
      "Average outlier percentage across datasets: 4.04%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def detect_outliers(df, contamination=0.05):\n",
    "    clf = IsolationForest(contamination=contamination, random_state=42)\n",
    "    preds = clf.fit_predict(df)\n",
    "    outliers = (preds == -1).sum()\n",
    "    total = len(df)\n",
    "    return (outliers / total) * 100\n",
    "\n",
    "def main():\n",
    "    datasets_folder = 'noise'\n",
    "    contamination = 0.05\n",
    "    outlier_percentages = []\n",
    "    dataset_count = 0\n",
    "\n",
    "    for filename in os.listdir(datasets_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(datasets_folder, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Drop label columns if they exist\n",
    "                label_columns = ['label', 'class', 'target', 'y']\n",
    "                df_features = df.copy()\n",
    "                for col in label_columns:\n",
    "                    if col in df_features.columns:\n",
    "                        df_features = df_features.drop(columns=[col])\n",
    "\n",
    "                # Keep only numeric columns\n",
    "                df_features = df_features.select_dtypes(include=[np.number])\n",
    "\n",
    "                # Skip datasets with no numeric features\n",
    "                if df_features.empty:\n",
    "                    continue\n",
    "\n",
    "                # Skip datasets with missing values\n",
    "                if df_features.isnull().any().any():\n",
    "                    continue\n",
    "\n",
    "                outlier_percentage = detect_outliers(df_features, contamination=contamination)\n",
    "                outlier_percentages.append(outlier_percentage)\n",
    "                dataset_count += 1\n",
    "\n",
    "            except Exception:\n",
    "                continue  # Just ignore problematic datasets silently\n",
    "\n",
    "    if outlier_percentages:\n",
    "        avg_outliers = np.mean(outlier_percentages)\n",
    "        print(f\"Number of datasets analyzed: {dataset_count}\")\n",
    "        print(f\"Average outlier percentage across datasets: {avg_outliers:.2f}%\")\n",
    "    else:\n",
    "        print(\"No valid datasets processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99a429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dab641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
